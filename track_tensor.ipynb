{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "from typing import Any, Callable, Dict\n",
    "\n",
    "\n",
    "np.seterr(divide=\"ignore\")\n",
    "\n",
    "\n",
    "def init_weights(init_fn: Callable[[int, int], float]) -> Callable[[nn.Module], None]:\n",
    "    def inner_fn(module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            fan_out, fan_in = module.weight.shape\n",
    "            module.weight.data.normal_(mean=0.0, std=init_fn(fan_in, fan_out))\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=1)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    return inner_fn\n",
    "\n",
    "\n",
    "def instrument(module: nn.Module) -> Dict[str, Dict[str, float]]:\n",
    "    stats: Dict[str, Dict[str, float]] = {}\n",
    "    instrument_recursive(module, stats)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def instrument_recursive(\n",
    "    module: nn.Module, stats: Dict[str, Dict[str, float]], name: str = \"\"\n",
    ") -> None:\n",
    "    children = list(module.named_children())\n",
    "    if children:\n",
    "        for c_name, c in children:\n",
    "            _name = f\"{name}.{c_name}\" if name and name != \"blocks\" else c_name\n",
    "            instrument_recursive(c, stats, _name)\n",
    "    else:\n",
    "        instrument_terminal(module, stats, name)\n",
    "\n",
    "\n",
    "def instrument_terminal(\n",
    "    module: nn.Module, stats: Dict[str, Dict[str, float]], name: str = \"\"\n",
    ") -> None:\n",
    "    module_stats: Dict[str, float] = {}\n",
    "\n",
    "    def require_input_grads(_module: nn.Module, input: Any) -> None:\n",
    "        for i in input:\n",
    "            if isinstance(i, Tensor) and i.is_floating_point():\n",
    "                i.requires_grad_()\n",
    "\n",
    "    module.register_forward_pre_hook(require_input_grads)\n",
    "\n",
    "    if name.split(\".\")[-1] == \"softmax\":\n",
    "        return\n",
    "\n",
    "    def record_fwd_scale(_module: nn.Module, input: Any, output: Any) -> None:\n",
    "        if isinstance(output, Tensor) and output.is_floating_point():\n",
    "            module_stats[\"x\"] = np.log2(output.std().item())\n",
    "\n",
    "    module.register_forward_hook(record_fwd_scale)\n",
    "\n",
    "    def record_bwd_scales(\n",
    "        _module: nn.Module, grad_input: Any, grad_output: Any\n",
    "    ) -> None:\n",
    "        grad_input = list(grad_input)\n",
    "        for g in grad_input:\n",
    "            if (\n",
    "                g is not None\n",
    "                and isinstance(g, Tensor)\n",
    "                and g.is_floating_point()\n",
    "                and len(grad_input) == 1\n",
    "            ):\n",
    "                module_stats[\"grad_x\"] = np.log2(g.std().item())\n",
    "\n",
    "        for param_name, param in _module.named_parameters():\n",
    "            if param_name == \"weight\":\n",
    "                module_stats[\"w\"] = np.log2(param.std().item())\n",
    "                if param.grad is not None:\n",
    "                    module_stats[\"grad_w\"] = np.log2(param.grad.std().item())\n",
    "\n",
    "    module.register_full_backward_hook(record_bwd_scales)\n",
    "\n",
    "    stats[name] = module_stats\n",
    "\n",
    "\n",
    "def visualise(stats: Dict[str, Dict[str, float]], subnormal: bool = False) -> None:\n",
    "    df = pd.DataFrame(stats)\n",
    "    df = df.stack().to_frame(\"scale (log₂)\").reset_index(names=[\"type\", \"op\"])\n",
    "    plot(df, subnormal)\n",
    "\n",
    "\n",
    "def plot(df: pd.DataFrame, subnormal: bool = False) -> None:\n",
    "    is_x_or_grad_x = (df[\"type\"] == \"x\") | (df[\"type\"] == \"grad_x\")\n",
    "    op_order = df[df[\"type\"] == \"x\"][\"op\"].tolist()\n",
    "    colors = [\"#6C8EBF\", \"#FF8000\", \"#5D8944\", \"#ED3434\"]\n",
    "    x_range = np.arange(-18 if subnormal else -14, 18 + 1 if subnormal else 16 + 1, 2)\n",
    "\n",
    "    fp16_min = alt.Chart().mark_rule(strokeDash=(4, 4)).encode(x=alt.datum(-14))\n",
    "    fp16_min_text = (\n",
    "        alt.Chart()\n",
    "        .mark_text(dy=-740)\n",
    "        .encode(text=alt.Text(value=\"Min FP16 (normal)\"), x=alt.datum(-10))\n",
    "    )\n",
    "    fp16_max = alt.Chart().mark_rule(strokeDash=(4, 4)).encode(x=alt.datum(16))\n",
    "    fp16_max_text = (\n",
    "        alt.Chart()\n",
    "        .mark_text(dy=-740)\n",
    "        .encode(text=alt.Text(value=\"Max FP16\"), x=alt.datum(13))\n",
    "    )\n",
    "\n",
    "    x_chart = (\n",
    "        alt.Chart(df[is_x_or_grad_x])\n",
    "        .mark_line()\n",
    "        .encode(\n",
    "            x=alt.X(\n",
    "                \"scale (log₂):Q\",\n",
    "                axis=alt.Axis(orient=\"top\", values=x_range),\n",
    "                scale=alt.Scale(domain=[x_range[0], x_range[-1]]),\n",
    "            ),\n",
    "            y=alt.Y(\"op:O\", title=\"\", sort=op_order),\n",
    "            color=alt.Color(\n",
    "                \"type\",\n",
    "                legend=alt.Legend(title=\"\", labelFontSize=12, symbolSize=100),\n",
    "                scale=alt.Scale(range=colors[:2]),\n",
    "                sort=\"descending\",\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    w_chart = (\n",
    "        alt.Chart(df[~is_x_or_grad_x])\n",
    "        .mark_point(size=100)\n",
    "        .encode(\n",
    "            x=alt.X(\n",
    "                \"scale (log₂):Q\",\n",
    "                axis=alt.Axis(orient=\"top\", values=x_range),\n",
    "                scale=alt.Scale(domain=[x_range[0], x_range[-1]]),\n",
    "            ),\n",
    "            y=alt.Y(\"op:O\", title=\"\", sort=op_order),\n",
    "            color=alt.Color(\n",
    "                \"type\",\n",
    "                legend=alt.Legend(title=\"\", labelFontSize=12, symbolSize=100),\n",
    "                scale=alt.Scale(range=colors[2:]),\n",
    "                sort=\"descending\",\n",
    "            ),\n",
    "            shape=alt.Shape(\n",
    "                \"type\",\n",
    "                scale=alt.Scale(range=[\"square\", \"triangle-down\"]),\n",
    "                sort=\"descending\",\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    layers = [x_chart, w_chart]\n",
    "    if subnormal:\n",
    "        layers += [fp16_min, fp16_max, fp16_min_text, fp16_max_text]\n",
    "    combined_chart = (\n",
    "        alt.layer(*layers)\n",
    "        .resolve_scale(color=\"independent\", shape=\"independent\")\n",
    "        .configure_axis(labelFontSize=12, titleFontSize=16)\n",
    "        .properties(width=500)\n",
    "    )\n",
    "    display(combined_chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_full_model(\n",
    "    model: nn.Module, batch_size: int = 64, seq_len: int = 16\n",
    ") -> None:\n",
    "    stats = instrument(model)\n",
    "    input_ids = labels = torch.randint(0, config.vocab_size, size=(batch_size, seq_len))\n",
    "    attention_mask = torch.zeros(batch_size, seq_len)\n",
    "    y = model(input_ids, attention_mask, labels=labels).loss\n",
    "    y.backward()\n",
    "    visualise(stats, subnormal=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
